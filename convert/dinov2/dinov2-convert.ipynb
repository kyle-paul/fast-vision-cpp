{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessor\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "processor.crop_size['height'] = 518\n",
    "processor.crop_size['width'] = 518\n",
    "processor.size['shortest_edge'] = 518\n",
    "\n",
    "# Load model\n",
    "dinov2_hf = AutoModel.from_pretrained('facebook/dinov2-base').eval().cuda() # 346M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward a sample image\n",
    "image = Image.open('/workspace/dataset/samples/truck.jpg')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "processed_image_hf = inputs[\"pixel_values\"].cuda()\n",
    "\n",
    "outputs = dinov2_hf(processed_image_hf)\n",
    "last_hidden_states = outputs.last_hidden_state  # shape (1, 1370, 768)\n",
    "vector_rep_hf = last_hidden_states[:, 0, :]     # shape (1, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters from BitImageProcessor\n",
    "crop_size = (518, 518)\n",
    "rescale_factor = 1.0 / 255.0\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "shortest_edge_size = 518\n",
    "\n",
    "def preprocessor(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    # Resize to 256\n",
    "    width, height = image.size\n",
    "    if width < height:\n",
    "        new_width = shortest_edge_size\n",
    "        new_height = int(shortest_edge_size * height / width)\n",
    "    else:\n",
    "        new_height = shortest_edge_size\n",
    "        new_width = int(shortest_edge_size * width / height)\n",
    "    image = image.resize((new_width, new_height), Image.BILINEAR)\n",
    "    \n",
    "    # Center crop to 224x224\n",
    "    left = (new_width - crop_size[0]) / 2\n",
    "    top = (new_height - crop_size[1]) / 2\n",
    "    right = (new_width + crop_size[0]) / 2\n",
    "    bottom = (new_height + crop_size[1]) / 2\n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    \n",
    "    # Convert image to numpy array and rescale\n",
    "    image_array = np.array(image) * rescale_factor\n",
    "    \n",
    "    # Normalize\n",
    "    image_array = (image_array - mean) / std\n",
    "    return image_array\n",
    "\n",
    "# Example usage\n",
    "processed_image = preprocessor('/workspace/dataset/samples/truck.jpg')\n",
    "processed_image = np.transpose(processed_image, (2, 0, 1))\n",
    "processed_image = torch.tensor(processed_image).unsqueeze(0).to(torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Difference: 0.06717944145202637\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity(tensor1, tensor2):\n",
    "    return torch.mean(torch.abs(tensor1 - tensor2))\n",
    "\n",
    "similarity = compute_similarity(processed_image_hf, processed_image)\n",
    "print(f\"Mean Absolute Difference: {similarity.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github TorchHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vitb14 = torch.hub.load(repo_or_dir='facebookresearch/dinov2', model='dinov2_vitb14').eval().cuda() # 330M\n",
    "vector_rep_hub = dinov2_vitb14(processed_image)  # shape (1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99993056"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two outputs are almost similar\n",
    "def similarity(a:np.ndarray, b:np.ndarray):\n",
    "    return (a @ b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "vector_rep_hf_np = vector_rep_hf.detach().cpu().numpy().reshape(-1, )\n",
    "vector_rep_hub_np = vector_rep_hub.detach().cpu().numpy().reshape(-1, )\n",
    "similarity(vector_rep_hf_np, vector_rep_hub_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Dinov2Model\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load the safetensors state_dict\n",
    "state_dict = load_file(\"/workspace/assets/models/dinov2/hugging-face/model.safetensors\")\n",
    "model_size = 'base'\n",
    "\n",
    "# Load the Dinov2Model with the state_dict and config\n",
    "dinov2_model, matching = Dinov2Model.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/workspace/assets/models/dinov2/hugging-face/\",\n",
    "    state_dict=state_dict,\n",
    "    output_loading_info=True,\n",
    "    config=\"/workspace/assets/configs/dinov2/config.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_model = dinov2_model.eval().cuda()\n",
    "output = dinov2_model(processed_image)\n",
    "last_hidden_states = output.last_hidden_state # (1, 257, 768)\n",
    "vector_rep_tr = last_hidden_states[:, 0, :]   # (1, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to ONNX and OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, dinov2_model):\n",
    "        super().__init__()\n",
    "        self.dinov2_model = dinov2_model\n",
    "    def forward(self, tensor):\n",
    "        return self.dinov2_model(tensor).last_hidden_state[:, 0, :]\n",
    "    \n",
    "model = Wrapper(dinov2_model).to('cpu')\n",
    "inp = torch.randn(1, 3, 224, 224)\n",
    "output = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_batch = {\n",
    "    'input'  : {0 : 'batch_size'},\n",
    "    'output' : {0 : 'batch_size'}\n",
    "}\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, torch.randn(1, 3, 518, 518), \n",
    "    \"/workspace/assets/models/dinov2/onnx/dinov2.onnx\",\n",
    "    export_params=True, do_constant_folding=True,\n",
    "    input_names=['input'], output_names=['output'],\n",
    "    dynamic_axes=dynamic_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocessor('/workspace/dataset/samples/truck.jpg')\n",
    "image = np.transpose(image, (2, 0, 1))[np.newaxis, ...]\n",
    "image = np.float32(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_cache_path = \"/workspace/assets/models/dinov2/onnx\"\n",
    "providers = [\n",
    "    ('TensorrtExecutionProvider', {\n",
    "        'device_id': 0,\n",
    "        'trt_max_workspace_size': 21474836480,\n",
    "        'trt_fp16_enable': False,\n",
    "        'trt_engine_cache_enable': True,\n",
    "        'trt_engine_cache_path': engine_cache_path}),\n",
    "    \n",
    "    ('CUDAExecutionProvider', {})    \n",
    "]\n",
    "\n",
    "dino_onnx = ort.InferenceSession(\"/workspace/assets/models/dinov2/onnx/dinov2.onnx\", providers=providers)\n",
    "dino_onnx_input = {\"input\": image}\n",
    "out_embedding = dino_onnx.run(None, dino_onnx_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 3.11 ms, total: 133 ms\n",
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out_embedding = dino_onnx.run(None, dino_onnx_input)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
